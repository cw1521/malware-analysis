{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create EMBER 2018 Malware Dataset <br>\n",
    "This program converts the ember malware database to a json format for export.<br>\n",
    "Install the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/elastic/ember.git pandas lightgbm lief"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import ember then create the vectorized features and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ember\n",
    "\n",
    "# ember.create_vectorized_features(\"../dataset/data/ember2018/\")\n",
    "# ember.create_metadata(\"../dataset/data/ember2018/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read metadata and vectorized features into lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = ember.read_metadata(\"../dataset/data/ember2018/\")\n",
    "metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = ember.read_vectorized_features(\"../dataset/data/ember2018/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details of x and y element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of Elements: {len(X_train)+len(X_test)}\")\n",
    "print(f\"X_train: {X_train[0]}\")\n",
    "print(f\"Number of columns in X_train: {len(X_train[0])}\")\n",
    "print(f\"y_train: {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the JSON version of the dataset to export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1951894445.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\school\\AppData\\Local\\Temp\\ipykernel_16348\\1951894445.py\"\u001b[1;36m, line \u001b[1;32m12\u001b[0m\n\u001b[1;33m    print(obj[\"x\"]))\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_data(df, x, y, size):\n",
    "    data_list = []\n",
    "    columns = df.columns\n",
    "    for i in range(size):\n",
    "        if i % 10000 == 0:\n",
    "            print (f\"Element: {i}\")\n",
    "        obj = {}    \n",
    "        ele = df.iloc[i]\n",
    "        obj[\"x\"] = x[i]\n",
    "        obj[\"y\"] = y[i]   \n",
    "        for col in columns:\n",
    "            if col == \"avclass\" and isinstance(ele[col], float):\n",
    "                obj[col] = str(ele[col])\n",
    "            elif col == \"label\":\n",
    "                obj[col] = str(ele[col])\n",
    "            else:\n",
    "                obj[col] = ele[col]\n",
    "        data_list.append(obj)\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = metadata_df[metadata_df[\"subset\"]==\"train\"]\n",
    "test_df = metadata_df[metadata_df[\"subset\"]==\"test\"]\n",
    "\n",
    "print(train_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(X_train)\n",
    "num_test = len(X_test)\n",
    "\n",
    "train_ds = get_data(train_df, X_train, y_train, num_train)\n",
    "test_ds = get_data(test_df, X_test, y_test, num_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" Special json encoder for numpy types \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def write_ds(ilist, num_div, task):\n",
    "    div_size = len(ilist) // num_div\n",
    "    file_number = 0\n",
    "    for i in range(0, len(ilist), div_size):\n",
    "        file_number += 1\n",
    "        if len(ilist[i:div_size+i]) == div_size:\n",
    "            ofile = [ilist[i:div_size+i], file_number, task]\n",
    "            write_list(ofile)\n",
    "        else:\n",
    "            ofile = [ilist[i:], file_number, task]\n",
    "            write_list(ofile)\n",
    "\n",
    "\n",
    "def write_list(ilist):\n",
    "    output_path = f\"{os.getcwd()}\\\\dataset\\\\data\\\\output\\\\ember2018_{str(ilist[2])}_{str(ilist[1])}.json\"\n",
    "    print(f\"Writing File: {ilist[1]}\")\n",
    "    with open(output_path, \"w\") as f: \n",
    "        ostr = json.dumps(ilist[0], cls=NumpyEncoder)           \n",
    "        f.write(ostr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the lists into a list for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = get_data(train_df, X_train, y_train, num_train)\n",
    "test_ds = get_data(test_df, X_test, y_test, num_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_div = 80\n",
    "train_type = \"train\"\n",
    "\n",
    "test_file_div = 20\n",
    "test_type = \"test\"\n",
    "\n",
    "write_ds(train_ds, train_file_div, train_type)\n",
    "write_ds(test_ds, train_file_div, train_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the file list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list(num_divs, data_type):\n",
    "    file_list = []\n",
    "    for i in range(num_divs):\n",
    "        file_str = f\"ember2018_{data_type}_{str(i+1)}.json\"\n",
    "        file_list.append(file_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list_path = f\"{os.getcwd()}\\\\dataset\\\\data\\\\output\\\\file_list.json\"\n",
    "\n",
    "file_list = {\n",
    "    \"train\": get_file_list(train_file_div, train_type),\n",
    "    \"test\": get_file_list(test_file_div, test_type)\n",
    "}\n",
    "\n",
    "with open(file_list_path, \"w\") as f:\n",
    "    json.dump(file_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Dataset Created.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
