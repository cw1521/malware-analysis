from json import dumps, JSONEncoder
from os import getcwd
from multiprocessing import Pool, cpu_count


# Create the JSON version of the dataset to export

def get_data(df, x, y, size):
    data = []
    num_proc = cpu_count()
    div_len = size // num_proc
    data_list = divide_data(df, x, y, div_len, size)
    results_list = []

    with Pool(num_proc) as p:
        results_list = p.map(get_obj_list, data_list)

    data = [obj for result in results_list for obj in result]
    return data



def divide_data(df, x, y, div_len, size):
    data_list = []
    for i in range(0, size, div_len):
        if size - i >= div_len:
            data = {
                "df": df.iloc[i:i+div_len],
                "x": x[i:i+div_len],
                "y": y[i:i+div_len]
            }
            data_list.append(data)
        else:
            data = {
                "df": df.iloc[i:i+div_len],
                "x": x[i:i+div_len],
                "y": y[i:i+div_len]
            }
            data_list.append(data)
    return data_list


def get_obj_list(data_list):
    obj_list = []
    for i in range(len(data_list["x"])):
        obj = get_obj(data_list["df"], i, data_list["x"], data_list["y"])
        obj_list.append(obj)
    return obj_list


def get_obj(df, i, x, y):
    obj = {}    
    ele = df.iloc[i]
    xi = " ".join(str(j) for j in x[i])
    obj["x"] = x[i]
    obj["input"] = xi
    obj["y"] = y[i] 
    columns = df.columns  
    for col in columns:
        if col == "avclass" and isinstance(ele[col], float):
            obj[col] = str(ele[col])
        elif col == "label":
            obj[col] = str(ele[col])
        else:
            obj[col] = ele[col]
    return obj



def write_ds(ilist, num_div, task):
    div_size = len(ilist) // num_div
    file_number = 0
    for i in range(0, len(ilist), div_size):
        file_number += 1
        if len(ilist[i:div_size+i]) == div_size:
            ofile = [ilist[i:div_size+i], file_number, task]
            write_list(ofile)
        else:
            ofile = [ilist[i:], file_number, task]
            write_list(ofile)



class NumpyEncoder(JSONEncoder):
    """ Special json encoder for numpy types """
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        return json.JSONEncoder.default(self, obj)



def write_list(ilist):
    output_path = f"{getcwd()}\\dataset\\data\\output\\ember2018_{str(ilist[2])}_{str(ilist[1])}.jsonl"
    print(f"Writing File: {ilist[1]}")
    ostr = dumps(ilist[0], indent=2, cls=NumpyEncoder)   
    with open(output_path, "w") as f:
        f.write(ostr)





if __name__ == "__main__":
    # Import ember then create the vectorized features and metadata

    import ember
    import numpy as np

    # Uncomment if metadata or vectors arent created
    # ember.create_vectorized_features("../data/ember2018/")
    # ember.create_metadata("../data/ember2018/")

    metadata_df = ember.read_metadata("../data/ember2018/")

    X_train, y_train, X_test, y_test = ember.read_vectorized_features("../data/ember2018/")

    train_df = metadata_df[metadata_df["subset"]=="train"]
    test_df = metadata_df[metadata_df["subset"]=="test"]

    num_train = len(X_train)
    num_test = len(X_test)

    train_ds = get_data(train_df, X_train, y_train, num_train)
    print(f"Train dataset created. Number of records {len(train_ds)}")
    
    test_ds = get_data(test_df, X_test, y_test, num_test)
    print(f"Test dataset created. Number of records {len(test_ds)}")

    train_file_div = 800
    train_type = "train"

    test_file_div = 200
    test_type = "test"

    write_ds(train_ds, train_file_div, train_type)
    write_ds(test_ds, test_file_div, test_type)

    print("Dataset Created.")