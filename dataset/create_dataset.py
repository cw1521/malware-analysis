from json import dumps, JSONEncoder
from os import getcwd, path, makedirs
from multiprocessing import Pool, cpu_count
# from threading import Thread
import numpy as np



# def get_threads(target, args, num_threads):
#     threads = []
#     for _ in range(num_threads):
#         threads.append(Thread(target=target, args=args))
#     return threads


# def start_threads(threads):
#     for t in threads:
#         t.start()

# def join_threads(threads):
#     for t in threads:
#         t.join()



def get_data(df, x, y, size):
    num_proc = get_num_procs()
    div_len = 10000
    data_list = divide_data(df, x, y, div_len, size)
    results_list = job(get_obj_list, data_list)
    data = [data for result in results_list for data in result]
    return data


def get_num_procs():
    # mod = cpu_count() // 8
    num_proc = cpu_count() // 4
    return num_proc


def divide_data(df, x, y, div_len, size):
    data_list = []
    for i in range(0, size, div_len):
        if size - i >= div_len:
            data = {
                "df": df.iloc[i:i+div_len],
                "x": x[i:i+div_len],
                "y": y[i:i+div_len]
            }
            data_list.append(data)
        else:
            data = {
                "df": df.iloc[i:i+div_len],
                "x": x[i:i+div_len],
                "y": y[i:i+div_len]
            }
            data_list.append(data)
    return data_list


def get_obj_list(data_list):
    obj_list = []
    for i in range(len(data_list["x"])):
        obj = get_obj(data_list["df"], i, data_list["x"], data_list["y"])
        obj_list.append(obj)
    return obj_list


def get_obj(df, i, x, y):
    obj = {}    
    ele = df.iloc[i]
    xi = " ".join(str(j) for j in x[i])
    obj["x"] = x[i]
    obj["input"] = xi
    obj["y"] = y[i] 
    columns = df.columns  
    for col in columns:
        if col == "avclass" and not isinstance(ele[col], str):
            obj[col] = str(ele[col])
        elif col == "label":
            obj[col] = str(ele[col])
        else:
            obj[col] = ele[col]
    return obj


def job(f, args_list):
    num_proc = get_num_procs()
    results_list = []
    with Pool(num_proc) as p:
        results_list = p.map(f, args_list)
    return results_list


def write_ds(ilist, num_div, task):
    file_list = get_file_list(ilist, num_div, task)
    results_list = job(get_output_list, file_list)
    output_list = [result for result in results_list]
    write_output_list(output_list)


def write_output_list(output_list):
    for output in output_list:
        print(f"Writing file: {output[0]}")
        with open(output[0], "w") as f:
            f.write(output[1])


def get_file_list(ilist, num_div, task):
    file_list = []
    div_len = len(ilist) // num_div
    file_number = 0
    for i in range(0, len(ilist), div_len):
        ofile = []
        file_number += 1
        if len(ilist) - i >= div_len:
            ofile = [ilist[i:div_len+i], file_number, task]
        else:
            ofile = [ilist[i:], file_number, task]
        file_list.append(ofile)
    return file_list


class NumpyEncoder(JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.integer):
            return str(obj)
        elif isinstance(obj, np.floating):
            return str(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        return JSONEncoder.default(self, obj)


def get_output_list(ilist):
    output_path = f"{getcwd()}\\dataset\\data\\output\\ember2018_{str(ilist[2])}_{str(ilist[1])}.jsonl"
    ostr = dumps(ilist[0], indent=2, cls=NumpyEncoder)   
    return [output_path, ostr]





def main():
    import ember

    output_dir = f"{getcwd()}\\output"

    # Uncomment if metadata or vectors arent created
    # ember.create_vectorized_features("../data/ember2018/")
    # ember.create_metadata("../data/ember2018/")

    metadata_df = ember.read_metadata("../data/ember2018/")

    X_train, y_train, X_test, y_test = ember.read_vectorized_features("../data/ember2018/")

    train_df = metadata_df[metadata_df["subset"]=="train"]
    test_df = metadata_df[metadata_df["subset"]=="test"]

    num_train = len(X_train)
    num_test = len(X_test)

    print("\nCreating the 'train' and 'test' split of the dataset.")
    train_ds = get_data(train_df, X_train, y_train, num_train)
    # print(train_ds[0])
    print(f"'Train' dataset created.\nNumber of records {len(train_ds)}.\nCreating Test dataset.")

    test_ds = get_data(test_df, X_test, y_test, num_test)
    # print(test_ds[0])
    print(f"'Test' dataset created.\nNumber of records {len(test_ds)}")

    train_file_div = 1000
    train_type = "train"

    test_file_div = 200
    test_type = "test"

    if not path.exists(output_dir):
        makedirs(output_dir)


    print("Serializing the 'train' dataset.")
    write_ds(train_ds, train_file_div, train_type)

    print("Serializing the 'test' dataset.")
    write_ds(test_ds, test_file_div, test_type)

    print("Datasets Created.")





if __name__ == "__main__":
    main()