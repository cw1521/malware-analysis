{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/elastic/ember.git\n",
      "  Cloning https://github.com/elastic/ember.git to c:\\users\\school\\appdata\\local\\temp\\pip-req-build-_8z5cbuh\n",
      "  Resolved https://github.com/elastic/ember.git to commit d97a0b523de02f3fe5ea6089d080abacab6ee931\n",
      "Requirement already satisfied: pandas in c:\\users\\school\\.conda\\envs\\test\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: lightgbm in c:\\users\\school\\.conda\\envs\\test\\lib\\site-packages (3.3.5)\n",
      "Requirement already satisfied: lief in c:\\users\\school\\.conda\\envs\\test\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\school\\.conda\\envs\\test\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\school\\.conda\\envs\\test\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\school\\.conda\\envs\\test\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\school\\.conda\\envs\\test\\lib\\site-packages (from lightgbm) (1.7.3)\n",
      "Requirement already satisfied: wheel in c:\\users\\school\\.conda\\envs\\test\\lib\\site-packages (from lightgbm) (0.37.1)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\school\\.conda\\envs\\test\\lib\\site-packages (from lightgbm) (1.0.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\school\\.conda\\envs\\test\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\school\\.conda\\envs\\test\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\school\\.conda\\envs\\test\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/elastic/ember.git 'C:\\Users\\school\\AppData\\Local\\Temp\\pip-req-build-_8z5cbuh'\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/elastic/ember.git pandas lightgbm lief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import dumps, JSONEncoder\n",
    "from os import getcwd, path, makedirs\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import numpy as np\n",
    "import ember\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data(df, x, y, size):\n",
    "    num_proc = get_num_procs()\n",
    "    div_len = size // num_proc\n",
    "    data_list = divide_data(df, x, y, div_len, size)\n",
    "    results_list = job(get_obj_list, data_list)\n",
    "    data = [data for result in results_list for data in result]\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_num_procs():\n",
    "    # mod = cpu_count() // 2\n",
    "    num_proc = cpu_count() // 8\n",
    "    return num_proc\n",
    "\n",
    "\n",
    "def divide_data(df, x, y, div_len, size):\n",
    "    data_list = []\n",
    "    for i in range(0, size, div_len):\n",
    "        if size - i >= div_len:\n",
    "            data = {\n",
    "                \"df\": df.iloc[i:i+div_len],\n",
    "                \"x\": x[i:i+div_len],\n",
    "                \"y\": y[i:i+div_len]\n",
    "            }\n",
    "            data_list.append(data)\n",
    "        else:\n",
    "            data = {\n",
    "                \"df\": df.iloc[i:i+div_len],\n",
    "                \"x\": x[i:i+div_len],\n",
    "                \"y\": y[i:i+div_len]\n",
    "            }\n",
    "            data_list.append(data)\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def get_obj_list(data_list):\n",
    "    obj_list = []\n",
    "    for i in range(len(data_list[\"x\"])):\n",
    "        obj = get_obj(data_list[\"df\"], i, data_list[\"x\"], data_list[\"y\"])\n",
    "        obj_list.append(obj)\n",
    "    return obj_list\n",
    "\n",
    "\n",
    "def get_obj(df, i, x, y):\n",
    "    obj = {}    \n",
    "    ele = df.iloc[i]\n",
    "    xi = \" \".join(str(j) for j in x[i])\n",
    "    obj[\"x\"] = x[i]\n",
    "    obj[\"input\"] = xi\n",
    "    obj[\"y\"] = y[i] \n",
    "    columns = df.columns  \n",
    "    for col in columns:\n",
    "        if col == \"label\":\n",
    "            obj[col] = str(ele[col])\n",
    "        else:\n",
    "            obj[col] = ele[col]\n",
    "    return obj\n",
    "\n",
    "\n",
    "def job(f, args_list):\n",
    "    num_proc = get_num_procs()\n",
    "    results_list = []\n",
    "    with Pool(num_proc) as p:\n",
    "        results_list = p.map(f, args_list)\n",
    "    return results_list\n",
    "\n",
    "\n",
    "def write_ds(ilist, num_div, task):\n",
    "    file_list = get_file_list(ilist, num_div, task)\n",
    "    results_list = job(get_output_list, file_list)\n",
    "    output_list = [result for result in results_list]\n",
    "    write_output_list(output_list)\n",
    "\n",
    "\n",
    "def write_output_list(output_list):\n",
    "    for output in output_list:\n",
    "        print(f\"Writing file: {output[0]}\")\n",
    "        with open(output[0], \"w\") as f:\n",
    "            f.write(output[1])\n",
    "\n",
    "\n",
    "def get_file_list(ilist, num_div, task):\n",
    "    file_list = []\n",
    "    # print(len(ilist))\n",
    "    div_len = len(ilist) // num_div\n",
    "    file_number = 0\n",
    "    for i in range(0, len(ilist), div_len):\n",
    "        ofile = []\n",
    "        file_number += 1\n",
    "        if len(ilist) - i >= div_size:\n",
    "            ofile = [ilist[i:div_size+i], file_number, task]\n",
    "        else:\n",
    "            ofile = [ilist[i:], file_number, task]\n",
    "        file_list.append(ofile)\n",
    "    return file_list\n",
    "\n",
    "\n",
    "class NumpyEncoder(JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return JSONEncoder.default(self, obj)\n",
    "\n",
    "\n",
    "def get_output_list(ilist):\n",
    "    output_path = f\"{getcwd()}\\\\dataset\\\\data\\\\output\\\\ember2018_{str(ilist[2])}_{str(ilist[1])}.jsonl\"\n",
    "    ostr = dumps(ilist[0], indent=2, cls=NumpyEncoder)   \n",
    "    return [output_path, ostr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\n",
      "WARNING:   lief version 0.13.2-2d9855fc found instead. There may be slight inconsistencies\n",
      "WARNING:   in the feature calculations.\n"
     ]
    }
   ],
   "source": [
    "output_dir = f\"{getcwd()}\\\\output\"\n",
    "\n",
    "# Uncomment if metadata or vectors arent created\n",
    "# ember.create_vectorized_features(\"../data/ember2018/\")\n",
    "# ember.create_metadata(\"../data/ember2018/\")\n",
    "\n",
    "metadata_df = ember.read_metadata(\"../data/ember2018/\")\n",
    "\n",
    "X_train, y_train, X_test, y_test = ember.read_vectorized_features(\"../data/ember2018/\")\n",
    "\n",
    "train_df = metadata_df[metadata_df[\"subset\"]==\"train\"]\n",
    "test_df = metadata_df[metadata_df[\"subset\"]==\"test\"]\n",
    "\n",
    "num_train = len(X_train)\n",
    "num_test = len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\nCreating the 'train' and 'test' split of the dataset.\")\n",
    "# train_ds = get_data(train_df, X_train, y_train, num_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"'Train' dataset created. Number of records {len(train_ds)}. Creating Test dataset.\")\n",
    "\n",
    "test_ds = get_data(test_df, X_test, y_test, num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_ds[0])\n",
    "print(f\"'Test' dataset created. Number of records {len(test_ds)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_file_div = 800\n",
    "# train_type = \"train\"\n",
    "\n",
    "test_file_div = 200\n",
    "test_type = \"test\"\n",
    "\n",
    "if not path.exists(output_dir):\n",
    "    makedirs(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"Serializing the 'train' dataset.\")\n",
    "# write_ds(train_ds, train_file_div, train_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Serializing the 'test' dataset.\")\n",
    "write_ds(test_ds, test_file_div, test_type)\n",
    "\n",
    "print(\"Datasets Created.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
