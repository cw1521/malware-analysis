from datasets import load_dataset
import numpy as np
import evaluate
from transformers import ( 
    AutoTokenizer,
    LongformerForSequenceClassification,
    Trainer,
    TrainingArguments
)






class ClassifierTrainer:


    def __init__(
            self,
            model_checkpoint,
            dataset_name,
            model_name,
            mode,
            num_epochs
        ):
        self.model_checkpoint = model_checkpoint
        self.dataset_name = dataset_name
        self.model_name = model_name
        self.num_epochs = num_epochs
        self.mode = mode
        self.set_mode()
        self.cols = [
            "subset", 
            "sha256",
            "appeared",
            "x",
            "y",
            "avclass"
        ]
        self.max_input_length = 4096
        self.max_output_length = 512

        self.accuracy = evaluate.load("accuracy")
        self.tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
        self.model = LongformerForSequenceClassification.from_pretrained(
            model_checkpoint,
            num_labels=1
        )
        self.trainer = self.get_trainer()

    def set_mode(self):
        if self.mode == "test":
            self.test = True
            self.batch_size = 1
        else:
            self.test = False
            self.batch_size = 16


    def get_dataset(self):
        dataset = None
        if not self.test:            
            dataset = load_dataset(
                self.dataset_name
            )
        else:
            dataset = load_dataset(
                self.dataset_name
            )
        dataset = dataset.remove_columns(self.cols)
        dataset = dataset["train"].train_test_split(test_size=0.2)
        train = dataset["train"]
        valid = dataset["test"]
        return train, valid
    

    def process_data_to_model_inputs(self, batch):
        inputs = self.tokenizer(
            batch["input"],
            truncation=True,
            padding="max_length",
            max_length=self.max_input_length
        )
        batch["input_ids"] = inputs.input_ids
        batch["attention_mask"] = inputs.attention_mask
        # create 0 global_attention_mask lists
        batch["global_attention_mask"] = len(batch["input_ids"]) * [
            [0 for _ in range(len(batch["input_ids"][0]))]
        ]
        # since above lists are references, the following 
        # line changes the 0 index for all samples
        batch["global_attention_mask"][0][0] = 1
        batch["labels"] = batch["label"]
        return batch


    def get_tokenized_dataset(self):
        train_ds, valid_ds = self.get_dataset()            
        train = train_ds.map(
            self.process_data_to_model_inputs,
            batch_size=self.batch_size,
            batched=True
        )
        valid = valid_ds.map(
            self.process_data_to_model_inputs,
            batch_size=self.batch_size,
            batched=True
        )
        train.set_format(
            type="torch",
            columns=["input_ids", "attention_mask", "global_attention_mask", "labels"]
        )
        valid.set_format(
            type="torch",
            columns=["input_ids", "attention_mask", "global_attention_mask", "labels"]
        )
        return valid, train


    def compute_metrics(self, eval_pred):
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=1)
        return self.accuracy.compute(predictions=predictions, references=labels)


    def get_training_args(self):
        args = TrainingArguments(
            self.model_name,
            evaluation_strategy="epoch",
            per_device_train_batch_size=self.batch_size,
            per_device_eval_batch_size=self.batch_size,
            learning_rate=1e-4,
            weight_decay=0.001,
            fp16=True,
            logging_dir='./logs',
            save_steps=100,
            save_total_limit=3,
            gradient_accumulation_steps=4,
            gradient_checkpointing=True,
            num_train_epochs=self.num_epochs
        )
        return args


    def get_trainer(self):
        train, valid = self.get_tokenized_dataset()
        training_args = self.get_training_args()
        trainer = Trainer(
            model=self.model,
            tokenizer=self.tokenizer,
            args=training_args,
            compute_metrics=self.compute_metrics,
            train_dataset=train,
            eval_dataset=valid,
        )
        return trainer


    def train(self):
        self.trainer.train()
        self.trainer.save_model()
        self.trainer.save_state()


