from datasets import load_dataset
import numpy as np
import evaluate
from transformers import ( 
    AutoTokenizer,
    AutoConfig,
    LongformerForSequenceClassification,
    Trainer,
    TrainingArguments
)






class Classifierrainer:
    data_files = [
        "..\\..\\ember2018\\data\\ember2018_train_1.jsonl",
        "..\\..\\ember2018\\data\\ember2018_test_1.jsonl"
    ]

    def __init__(
            self,
            model_checkpoint,
            dataset_name,
            model_name,
            num_epochs,
            mode
        ):
        return



















dataset_name = "cw1521/ember2018-malware"
model_checkpoint = "allenai/longformer-base-4096"
model_name = "ma-ember-1"



# dataset = load_dataset(
#     dataset_name,
#     split="train"
# )





dataset = load_dataset(
    "json",
    data_files=data_files
)


cols = [
    "subset", 
    "sha256",
    "appeared",
    "x",
    "y",
    "avclass"
]


dataset = dataset.remove_columns(cols)








dataset = dataset["train"].train_test_split(test_size=0.2)

train_ds = dataset["train"]
valid_ds = dataset["test"]







config = AutoConfig.from_pretrained(
    model_checkpoint,
    num_labels=1
)
model = LongformerForSequenceClassification.from_pretrained(
    model_checkpoint,
    config=config
)
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

max_input_length = 4096
max_output_length = 512
batch_size = 1




def process_data_to_model_inputs(batch):
    inputs = tokenizer(
        batch["input"],
        truncation=True,
        padding="max_length",
        max_length=max_input_length
    )

    batch["input_ids"] = inputs.input_ids
    batch["attention_mask"] = inputs.attention_mask

    # create 0 global_attention_mask lists
    batch["global_attention_mask"] = len(batch["input_ids"]) * [
        [0 for _ in range(len(batch["input_ids"][0]))]
    ]

    # since above lists are references, the following line changes the 0 index for all samples
    batch["global_attention_mask"][0][0] = 1

    batch["labels"] = batch["label"]
    return batch



train = train_ds.map(
    process_data_to_model_inputs,
    batch_size=batch_size,
    batched=True
)






valid = valid_ds.map(
    process_data_to_model_inputs,
    batch_size=batch_size,
    batched=True
)




train.set_format(
    type="torch",
    columns=["input_ids", "attention_mask", "global_attention_mask", "labels"]
)
valid.set_format(
    type="torch",
    columns=["input_ids", "attention_mask", "global_attention_mask", "labels"]
)




accuracy = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return accuracy.compute(predictions=predictions, references=labels)



training_args = TrainingArguments(
    model_name,
    evaluation_strategy="epoch",
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    learning_rate=1e-4,
    weight_decay=0.001,
    fp16=True,
    logging_dir='./logs',
    save_steps=100,
    save_total_limit=3,
    gradient_accumulation_steps=4,
    gradient_checkpointing=True,
    num_train_epochs=1
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    compute_metrics=compute_metrics,
    train_dataset=train,
    eval_dataset=valid,
)





trainer.train()
trainer.save_model()
trainer.save_state()









