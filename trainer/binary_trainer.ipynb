{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGIXLVSOTDYZ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRYeuqagTDYb",
        "outputId": "a811132f-de6e-41d0-807c-5399635846d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: datasets in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages (2.5.1)\n",
            "Requirement already satisfied: transformers[torch] in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages (4.22.1)\n",
            "Requirement already satisfied: rouge_score in c:\\users\\school\\appdata\\roaming\\python\\python310\\site-packages (0.1.2)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages (from datasets) (1.23.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages (from datasets) (8.0.0)\n",
            "Requirement already satisfied: dill<0.3.6 in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages (from datasets) (1.4.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages (from datasets) (2.28.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages (from datasets) (4.64.0)\n",
            "Requirement already satisfied: xxhash in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages (from datasets) (0.0.0)\n",
            "Requirement already satisfied: multiprocess in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages (from datasets) (2022.8.2)\n",
            "Requirement already satisfied: aiohttp in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages\\huggingface_hub-0.10.0rc3-py3.8.egg (from datasets) (0.10.0rc3)\n",
            "Requirement already satisfied: packaging in c:\\users\\school\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: responses<0.19 in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages (from transformers[torch]) (3.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages (from transformers[torch]) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages (from transformers[torch]) (2022.7.9)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages (from transformers[torch]) (0.11.4)\n",
            "Requirement already satisfied: torch!=0.12.0,>=1.0 in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages (from transformers[torch]) (1.12.1)\n",
            "Requirement already satisfied: six>=1.14.0 in c:\\users\\school\\appdata\\roaming\\python\\python310\\site-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: nltk in c:\\users\\school\\appdata\\roaming\\python\\python310\\site-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: absl-py in c:\\users\\school\\appdata\\roaming\\python\\python310\\site-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\school\\appdata\\roaming\\python\\python310\\site-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.11)\n",
            "Requirement already satisfied: colorama in c:\\users\\school\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.62.1->datasets) (0.4.5)\n",
            "Requirement already satisfied: joblib in c:\\users\\school\\appdata\\roaming\\python\\python310\\site-packages (from nltk->rouge_score) (1.3.1)\n",
            "Requirement already satisfied: click in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages (from nltk->rouge_score) (8.0.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\envs\\dev\\lib\\site-packages (from pandas->datasets) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\school\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets) (2.8.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install datasets transformers[torch] rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1sVuKs4dTDYe"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"cw1521/ember2018-malware\"\n",
        "model_checkpoint = \"allenai/led-base-16384\"\n",
        "model_name = \"ma-ember-1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ru1EJi89TDYf",
        "outputId": "a2df5533-b971-4723-e297-1828680c760d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom data configuration default-920c1cb545b0ae85\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset json/default to C:/Users/school/.cache/huggingface/datasets/json/default-920c1cb545b0ae85/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08e4108934f84f80b318e5c17dca8265",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6fcda5eeda944965a77b15f9187adb7d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5282fa19245f46c7aeb190fe4cbedf47",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0 tables [00:00, ? tables/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset json downloaded and prepared to C:/Users/school/.cache/huggingface/datasets/json/default-920c1cb545b0ae85/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5482171e7574702a4f2a45de3da55c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "\n",
        "# dataset = load_dataset(\n",
        "#     dataset_name,\n",
        "#     split=\"train\",\n",
        "#     field=\"data\"\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testing (comment out for full dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_files = [\n",
        "    \"..\\\\dataset\\\\data\\\\db\\\\ember2018_train_1.jsonl\",\n",
        "    \"..\\\\dataset\\\\data\\\\db\\\\ember2018_test_1.jsonl\"\n",
        "]\n",
        "\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=data_files,\n",
        "    field=\"data\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67Hxkt__Uy3j",
        "outputId": "8bcd261a-06eb-4112-8bb9-e2b46828633a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['x', 'y', 'sha256', 'appeared', 'label', 'avclass', 'subset'],\n",
              "        num_rows: 20000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "KELXMCHIotKA",
        "outputId": "d37da554-8d80-4d3a-c668-e392a4ef3877"
      },
      "outputs": [],
      "source": [
        "dataset = dataset[\"train\"].train_test_split(test_size=0.2)\n",
        "\n",
        "train_ds = dataset[\"train\"]\n",
        "valid_ds = dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "IPyAGTOTUnui"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVyQYNpoTDYf"
      },
      "source": [
        "Set max_input_length, max_output_length, and batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Bn_Z2Jp6TDYg"
      },
      "outputs": [],
      "source": [
        "max_input_length = 8192\n",
        "max_output_length = 512\n",
        "batch_size = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXLVQ2FfTDYg"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "TYxVKHQoTDYh"
      },
      "outputs": [],
      "source": [
        "def process_data_to_model_inputs(batch):\n",
        "    # tokenize the inputs and labels\n",
        "\n",
        "    x = [str(i) for i in batch['x']]\n",
        "    y = [str(i) for i in batch['y']]\n",
        "    \n",
        "    inputs = tokenizer(\n",
        "        x,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_input_length,\n",
        "    )\n",
        "    outputs = tokenizer(\n",
        "        y,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_output_length,\n",
        "    )\n",
        "\n",
        "    batch[\"input_ids\"] = inputs.input_ids\n",
        "    batch[\"attention_mask\"] = inputs.attention_mask\n",
        "\n",
        "    # create 0 global_attention_mask lists\n",
        "    batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [\n",
        "        [0 for _ in range(len(batch[\"input_ids\"][0]))]\n",
        "    ]\n",
        "\n",
        "    # since above lists are references, the following line changes the 0 index for all samples\n",
        "    batch[\"global_attention_mask\"][0][0] = 1\n",
        "    batch[\"labels\"] = outputs.input_ids\n",
        "\n",
        "    # We have to make sure that the PAD token is ignored\n",
        "    batch[\"labels\"] = [\n",
        "        [-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]\n",
        "    ]\n",
        "\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iWcGExmTDYi"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "f99a351e40174777972522996bc3f1c1",
            "9369172028c847f183725c3d9cc41ddb",
            "413aab8aaca4461fbbedfc8145f6c9b7",
            "943dcd3d27764207b0d088e7d7473ab9",
            "31c9ccd453424af283cb4a7f043cffba",
            "f8b9044b1ed84be8878012743e2672d6",
            "109b97e9cf5f476b9b81ea0dbdf31d4c",
            "2eae9b35d22840fd82bd64eac0731d98",
            "3d5a5479e47f464c9553e79e6eb62908",
            "c04d2de91a1d4a84a9876f770974b238",
            "1e34ddc9223b43d38f343ef4b223d384"
          ]
        },
        "id": "Uj9ANQgRTDYi",
        "outputId": "f9b046a6-f4b1-4b86-9c0e-34284c2ff305"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "758bc1053df847f8b10a9c28d5bab623",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/8000 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train = train_ds.map(\n",
        "    process_data_to_model_inputs,\n",
        "    batched=True,\n",
        "    batch_size=batch_size,\n",
        "    remove_columns=[\"x\", \"y\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b5db6139ce704ef5982f71a8a159fe58",
            "a7f5b1fe78be4f62b8a40cea8d54256b",
            "97aca7406cfd4204b0f958334e14bd00",
            "59b0cd80aad44db2b017d71c8d29f5c0",
            "d4254f8c4f6d4961892e4cd5c475846c",
            "8fa40420ef4d46a5bcce25a2363fbf42",
            "006bf7d019b348e7b349a5d1ef369489",
            "6315bd32aafa436094c21e959d8781f1",
            "311d5c35be7e4ffa8dc7e68ef63d7be5",
            "99b6ce6a2669440aa517a76f911bd711",
            "c43d3d1d3ee74e27ae1fde4ffca0a2a2"
          ]
        },
        "id": "Ne5PAeWfTDYi",
        "outputId": "13e546f4-9664-4a94-ff5c-80ac2291a017"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "709cee6c2ac944f2b4f915b02bb42108",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2000 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "valid = valid_ds.map(\n",
        "    process_data_to_model_inputs,\n",
        "    batched=True,\n",
        "    batch_size=batch_size,\n",
        "    remove_columns=[\"x\", \"y\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihH80YxyTDYj"
      },
      "source": [
        "Convert the dataset to torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "IjMt0_J9TDYj"
      },
      "outputs": [],
      "source": [
        "train.set_format(\n",
        "    type=\"torch\",\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
        ")\n",
        "valid.set_format(\n",
        "    type=\"torch\",\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNWlzTnGTDYj"
      },
      "source": [
        "Model Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "W5se9Yo4TDYj"
      },
      "outputs": [],
      "source": [
        "model.config.num_beams = 2\n",
        "model.config.max_length = 512\n",
        "model.config.min_length = 100\n",
        "model.config.length_penalty = 2.0\n",
        "model.config.early_stopping = True\n",
        "model.config.no_repeat_ngram_size = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeDbmafUTDYk"
      },
      "source": [
        "Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "a2p1QFJzTDYk"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\school\\AppData\\Local\\Temp\\ipykernel_20720\\2688135933.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  rouge = load_metric(\"rouge\")\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "280dbfd4f06a4661b46b87ee3d13f4a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/2.16k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "rouge = load_metric(\"rouge\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
        "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "\n",
        "    rouge_output = rouge.compute(\n",
        "        predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"]\n",
        "    )[\"rouge2\"].mid\n",
        "\n",
        "    return {\n",
        "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
        "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
        "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4)\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAS0BqBPTDYk"
      },
      "source": [
        "Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "xH5w2zJ4TDYk"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    model_name,\n",
        "    predict_with_generate=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    learning_rate=1e-4,\n",
        "    weight_decay=1e-5,\n",
        "    tf32=True,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=5,\n",
        "    eval_steps=10,\n",
        "    save_steps=10,\n",
        "    save_total_limit=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=True,\n",
        "    num_train_epochs=1,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train,\n",
        "    eval_dataset=valid,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "T0w69n6jTDYl"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: subset, appeared, avclass, sha256. If subset, appeared, avclass, sha256 are not expected by `LEDForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "c:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 16000\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 4\n",
            "  Total optimization steps = 2000\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6632fbc2184b46c3be93438198c70b56",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a LEDTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "CUDA out of memory. Tried to allocate 1.41 GiB (GPU 0; 4.00 GiB total capacity; 2.83 GiB already allocated; 0 bytes free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\school\\code\\malware-analysis\\trainer\\binary_trainer.ipynb Cell 26\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/school/code/malware-analysis/trainer/binary_trainer.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/school/code/malware-analysis/trainer/binary_trainer.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39msave_model()\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\trainer.py:1521\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1518\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1519\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1520\u001b[0m )\n\u001b[1;32m-> 1521\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1522\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1523\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1524\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1525\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1526\u001b[0m )\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\trainer.py:1763\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1761\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1762\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1763\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1765\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1766\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1767\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1768\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1769\u001b[0m ):\n\u001b[0;32m   1770\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1771\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\trainer.py:2499\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2496\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m   2498\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2499\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[0;32m   2501\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   2502\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2529\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2530\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2531\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m   2532\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2533\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2534\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\models\\led\\modeling_led.py:2436\u001b[0m, in \u001b[0;36mLEDForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, global_attention_mask, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   2431\u001b[0m     \u001b[39mif\u001b[39;00m decoder_input_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2432\u001b[0m         decoder_input_ids \u001b[39m=\u001b[39m shift_tokens_right(\n\u001b[0;32m   2433\u001b[0m             labels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m   2434\u001b[0m         )\n\u001b[1;32m-> 2436\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mled(\n\u001b[0;32m   2437\u001b[0m     input_ids,\n\u001b[0;32m   2438\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   2439\u001b[0m     decoder_input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[0;32m   2440\u001b[0m     decoder_attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[0;32m   2441\u001b[0m     encoder_outputs\u001b[39m=\u001b[39;49mencoder_outputs,\n\u001b[0;32m   2442\u001b[0m     global_attention_mask\u001b[39m=\u001b[39;49mglobal_attention_mask,\n\u001b[0;32m   2443\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   2444\u001b[0m     decoder_head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[0;32m   2445\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[0;32m   2446\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   2447\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   2448\u001b[0m     decoder_inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[0;32m   2449\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   2450\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   2451\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   2452\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   2453\u001b[0m )\n\u001b[0;32m   2454\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(outputs[\u001b[39m0\u001b[39m]) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_logits_bias\n\u001b[0;32m   2456\u001b[0m masked_lm_loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\models\\led\\modeling_led.py:2282\u001b[0m, in \u001b[0;36mLEDModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, global_attention_mask, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   2277\u001b[0m     decoder_input_ids \u001b[39m=\u001b[39m shift_tokens_right(\n\u001b[0;32m   2278\u001b[0m         input_ids, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m   2279\u001b[0m     )\n\u001b[0;32m   2281\u001b[0m \u001b[39mif\u001b[39;00m encoder_outputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 2282\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   2283\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m   2284\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   2285\u001b[0m         global_attention_mask\u001b[39m=\u001b[39;49mglobal_attention_mask,\n\u001b[0;32m   2286\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   2287\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   2288\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   2289\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   2290\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   2291\u001b[0m     )\n\u001b[0;32m   2292\u001b[0m \u001b[39m# If the user passed a tuple for encoder_outputs, we wrap it in a LEDEncoderBaseModelOutput when return_dict=False\u001b[39;00m\n\u001b[0;32m   2293\u001b[0m \u001b[39melif\u001b[39;00m return_dict \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(encoder_outputs, LEDEncoderBaseModelOutput):\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\models\\led\\modeling_led.py:1890\u001b[0m, in \u001b[0;36mLEDEncoder.forward\u001b[1;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1886\u001b[0m             \u001b[39mreturn\u001b[39;00m module(\u001b[39m*\u001b[39minputs, is_global_attn, output_attentions)\n\u001b[0;32m   1888\u001b[0m         \u001b[39mreturn\u001b[39;00m custom_forward\n\u001b[1;32m-> 1890\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mcheckpoint\u001b[39m.\u001b[39;49mcheckpoint(\n\u001b[0;32m   1891\u001b[0m         create_custom_forward(encoder_layer),\n\u001b[0;32m   1892\u001b[0m         hidden_states,\n\u001b[0;32m   1893\u001b[0m         attention_mask,\n\u001b[0;32m   1894\u001b[0m         head_mask[idx] \u001b[39mif\u001b[39;49;00m head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   1895\u001b[0m         is_index_masked,\n\u001b[0;32m   1896\u001b[0m         is_index_global_attn,\n\u001b[0;32m   1897\u001b[0m     )\n\u001b[0;32m   1898\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1899\u001b[0m     layer_outputs \u001b[39m=\u001b[39m encoder_layer(\n\u001b[0;32m   1900\u001b[0m         hidden_states,\n\u001b[0;32m   1901\u001b[0m         attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1906\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   1907\u001b[0m     )\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\torch\\utils\\checkpoint.py:235\u001b[0m, in \u001b[0;36mcheckpoint\u001b[1;34m(function, use_reentrant, *args, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnexpected keyword arguments: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(arg \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m kwargs))\n\u001b[0;32m    234\u001b[0m \u001b[39mif\u001b[39;00m use_reentrant:\n\u001b[1;32m--> 235\u001b[0m     \u001b[39mreturn\u001b[39;00m CheckpointFunction\u001b[39m.\u001b[39;49mapply(function, preserve, \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    236\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    237\u001b[0m     \u001b[39mreturn\u001b[39;00m _checkpoint_without_reentrant(\n\u001b[0;32m    238\u001b[0m         function,\n\u001b[0;32m    239\u001b[0m         preserve,\n\u001b[0;32m    240\u001b[0m         \u001b[39m*\u001b[39margs\n\u001b[0;32m    241\u001b[0m     )\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\torch\\utils\\checkpoint.py:96\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[1;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[0;32m     93\u001b[0m ctx\u001b[39m.\u001b[39msave_for_backward(\u001b[39m*\u001b[39mtensor_inputs)\n\u001b[0;32m     95\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 96\u001b[0m     outputs \u001b[39m=\u001b[39m run_function(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m     97\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\models\\led\\modeling_led.py:1886\u001b[0m, in \u001b[0;36mLEDEncoder.forward.<locals>.create_custom_forward.<locals>.custom_forward\u001b[1;34m(*inputs)\u001b[0m\n\u001b[0;32m   1885\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcustom_forward\u001b[39m(\u001b[39m*\u001b[39minputs):\n\u001b[1;32m-> 1886\u001b[0m     \u001b[39mreturn\u001b[39;00m module(\u001b[39m*\u001b[39;49minputs, is_global_attn, output_attentions)\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\models\\led\\modeling_led.py:976\u001b[0m, in \u001b[0;36mLEDEncoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[0;32m    967\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    968\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    969\u001b[0m \u001b[39m    hidden_states (`torch.FloatTensor`): input to the layer of shape *(seq_len, batch, embed_dim)*\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[39m        *(encoder_attention_heads,)*.\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    975\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m--> 976\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[0;32m    977\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[0;32m    978\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    979\u001b[0m     layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[0;32m    980\u001b[0m     is_index_masked\u001b[39m=\u001b[39;49mis_index_masked,\n\u001b[0;32m    981\u001b[0m     is_index_global_attn\u001b[39m=\u001b[39;49mis_index_global_attn,\n\u001b[0;32m    982\u001b[0m     is_global_attn\u001b[39m=\u001b[39;49mis_global_attn,\n\u001b[0;32m    983\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    984\u001b[0m )\n\u001b[0;32m    985\u001b[0m hidden_states \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    986\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(hidden_states, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\models\\led\\modeling_led.py:785\u001b[0m, in \u001b[0;36mLEDEncoderAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    774\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    775\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    781\u001b[0m     output_attentions: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    782\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, Optional[torch\u001b[39m.\u001b[39mTensor], Optional[Tuple[torch\u001b[39m.\u001b[39mTensor]]]:\n\u001b[0;32m    783\u001b[0m     \u001b[39m\"\"\"Input shape: Batch x Time x Channel\"\"\"\u001b[39;00m\n\u001b[1;32m--> 785\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlongformer_self_attn(\n\u001b[0;32m    786\u001b[0m         hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[0;32m    787\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    788\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[0;32m    789\u001b[0m         is_index_masked\u001b[39m=\u001b[39;49mis_index_masked,\n\u001b[0;32m    790\u001b[0m         is_index_global_attn\u001b[39m=\u001b[39;49mis_index_global_attn,\n\u001b[0;32m    791\u001b[0m         is_global_attn\u001b[39m=\u001b[39;49mis_global_attn,\n\u001b[0;32m    792\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    793\u001b[0m     )\n\u001b[0;32m    795\u001b[0m     attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m])\n\u001b[0;32m    796\u001b[0m     outputs \u001b[39m=\u001b[39m (attn_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\models\\led\\modeling_led.py:203\u001b[0m, in \u001b[0;36mLEDEncoderSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[0;32m    200\u001b[0m query_vectors \u001b[39m=\u001b[39m query_vectors\u001b[39m.\u001b[39mview(seq_len, batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m    201\u001b[0m key_vectors \u001b[39m=\u001b[39m key_vectors\u001b[39m.\u001b[39mview(seq_len, batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m--> 203\u001b[0m attn_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sliding_chunks_query_key_matmul(\n\u001b[0;32m    204\u001b[0m     query_vectors, key_vectors, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mone_sided_attn_window_size\n\u001b[0;32m    205\u001b[0m )\n\u001b[0;32m    207\u001b[0m \u001b[39m# values to pad for attention probs\u001b[39;00m\n\u001b[0;32m    208\u001b[0m remove_from_windowed_attention_mask \u001b[39m=\u001b[39m (attention_mask \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m)[:, :, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m]\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\models\\led\\modeling_led.py:481\u001b[0m, in \u001b[0;36mLEDEncoderSelfAttention._sliding_chunks_query_key_matmul\u001b[1;34m(self, query, key, window_overlap)\u001b[0m\n\u001b[0;32m    478\u001b[0m diagonal_chunked_attention_scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meinsum(\u001b[39m\"\u001b[39m\u001b[39mbcxd,bcyd->bcxy\u001b[39m\u001b[39m\"\u001b[39m, (query, key))  \u001b[39m# multiply\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \u001b[39m# convert diagonals into columns\u001b[39;00m\n\u001b[1;32m--> 481\u001b[0m diagonal_chunked_attention_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pad_and_transpose_last_two_dims(\n\u001b[0;32m    482\u001b[0m     diagonal_chunked_attention_scores, padding\u001b[39m=\u001b[39;49m(\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m)\n\u001b[0;32m    483\u001b[0m )\n\u001b[0;32m    485\u001b[0m \u001b[39m# allocate space for the overall attention matrix where the chunks are combined. The last dimension\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[39m# has (window_overlap * 2 + 1) columns. The first (window_overlap) columns are the window_overlap lower triangles (attention from a word to\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[39m# window_overlap previous words). The following column is attention score from each word to itself, then\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[39m# followed by window_overlap columns for the upper triangle.\u001b[39;00m\n\u001b[0;32m    490\u001b[0m diagonal_attention_scores \u001b[39m=\u001b[39m diagonal_chunked_attention_scores\u001b[39m.\u001b[39mnew_zeros(\n\u001b[0;32m    491\u001b[0m     (batch_size \u001b[39m*\u001b[39m num_heads, chunks_count \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, window_overlap, window_overlap \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m    492\u001b[0m )\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\models\\led\\modeling_led.py:336\u001b[0m, in \u001b[0;36mLEDEncoderSelfAttention._pad_and_transpose_last_two_dims\u001b[1;34m(hidden_states_padded, padding)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_pad_and_transpose_last_two_dims\u001b[39m(hidden_states_padded, padding):\n\u001b[0;32m    335\u001b[0m     \u001b[39m\"\"\"pads rows and then flips rows and columns\"\"\"\u001b[39;00m\n\u001b[1;32m--> 336\u001b[0m     hidden_states_padded \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mpad(\n\u001b[0;32m    337\u001b[0m         hidden_states_padded, padding\n\u001b[0;32m    338\u001b[0m     )  \u001b[39m# padding value is not important because it will be overwritten\u001b[39;00m\n\u001b[0;32m    339\u001b[0m     hidden_states_padded \u001b[39m=\u001b[39m hidden_states_padded\u001b[39m.\u001b[39mview(\n\u001b[0;32m    340\u001b[0m         \u001b[39m*\u001b[39mhidden_states_padded\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m], hidden_states_padded\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), hidden_states_padded\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m    341\u001b[0m     )\n\u001b[0;32m    342\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states_padded\n",
            "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.41 GiB (GPU 0; 4.00 GiB total capacity; 2.83 GiB already allocated; 0 bytes free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "trainer.train()\n",
        "trainer.save_model()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "006bf7d019b348e7b349a5d1ef369489": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "109b97e9cf5f476b9b81ea0dbdf31d4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e34ddc9223b43d38f343ef4b223d384": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2eae9b35d22840fd82bd64eac0731d98": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "311d5c35be7e4ffa8dc7e68ef63d7be5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "31c9ccd453424af283cb4a7f043cffba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "3d5a5479e47f464c9553e79e6eb62908": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "413aab8aaca4461fbbedfc8145f6c9b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2eae9b35d22840fd82bd64eac0731d98",
            "max": 8000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3d5a5479e47f464c9553e79e6eb62908",
            "value": 8000
          }
        },
        "59b0cd80aad44db2b017d71c8d29f5c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99b6ce6a2669440aa517a76f911bd711",
            "placeholder": "​",
            "style": "IPY_MODEL_c43d3d1d3ee74e27ae1fde4ffca0a2a2",
            "value": " 134/2000 [00:05&lt;00:53, 35.09 examples/s]"
          }
        },
        "6315bd32aafa436094c21e959d8781f1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fa40420ef4d46a5bcce25a2363fbf42": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9369172028c847f183725c3d9cc41ddb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8b9044b1ed84be8878012743e2672d6",
            "placeholder": "​",
            "style": "IPY_MODEL_109b97e9cf5f476b9b81ea0dbdf31d4c",
            "value": "Map: 100%"
          }
        },
        "943dcd3d27764207b0d088e7d7473ab9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c04d2de91a1d4a84a9876f770974b238",
            "placeholder": "​",
            "style": "IPY_MODEL_1e34ddc9223b43d38f343ef4b223d384",
            "value": " 7998/8000 [04:35&lt;00:00, 38.47 examples/s]"
          }
        },
        "97aca7406cfd4204b0f958334e14bd00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6315bd32aafa436094c21e959d8781f1",
            "max": 2000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_311d5c35be7e4ffa8dc7e68ef63d7be5",
            "value": 134
          }
        },
        "99b6ce6a2669440aa517a76f911bd711": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7f5b1fe78be4f62b8a40cea8d54256b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fa40420ef4d46a5bcce25a2363fbf42",
            "placeholder": "​",
            "style": "IPY_MODEL_006bf7d019b348e7b349a5d1ef369489",
            "value": "Map:   7%"
          }
        },
        "b5db6139ce704ef5982f71a8a159fe58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a7f5b1fe78be4f62b8a40cea8d54256b",
              "IPY_MODEL_97aca7406cfd4204b0f958334e14bd00",
              "IPY_MODEL_59b0cd80aad44db2b017d71c8d29f5c0"
            ],
            "layout": "IPY_MODEL_d4254f8c4f6d4961892e4cd5c475846c"
          }
        },
        "c04d2de91a1d4a84a9876f770974b238": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c43d3d1d3ee74e27ae1fde4ffca0a2a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4254f8c4f6d4961892e4cd5c475846c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8b9044b1ed84be8878012743e2672d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f99a351e40174777972522996bc3f1c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9369172028c847f183725c3d9cc41ddb",
              "IPY_MODEL_413aab8aaca4461fbbedfc8145f6c9b7",
              "IPY_MODEL_943dcd3d27764207b0d088e7d7473ab9"
            ],
            "layout": "IPY_MODEL_31c9ccd453424af283cb4a7f043cffba"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
